---
created: 2025-12-24 18:01:11
source: NotebookLM
tags: [notebooklm, imported]
---

# Ollama 기반 로컬 AI 구축 및 모바일 최적화 가이드

제공된 자료는 Ollama를 활용한 로컬 AI 실행 환경, 하드웨어 요구 사양, 안드로이드 기기(특히 갤럭시 노트 10 시리즈)에서의 구동 방법 및 최적화 전략에 대한 방대한 정보를 담고 있습니다. 주요 내용을 주제별로 정리해 드립니다.
--------------------------------------------------------------------------------
1. Ollama 및 로컬 LLM 하드웨어 요구 사양
로컬에서 대규모 언어 모델(LLM)을 원활하게 구동하기 위해서는 **VRAM(비디오 램)**과 시스템 RAM 확보가 가장 중요합니다.12

| 모델 규모 | 권장 RAM (Ollama 기준) | 권장 VRAM (GPU 가속 시) | 적합한 모델 예시 |
| --- | --- | --- | --- |
| 소형 (1B~4B) | 4GB~8GB34 | 3GB~4GB5 | Llama 3.2 1B/3B, Gemma 3 1B/4B, Phi-3 Mini67 |
| 중형 (7B~9B) | 8GB 이상8 | 6GB~8GB5 | Llama 3.1 8B, Qwen3 8B, Mistral 7B69 |
| 대형 (12B~35B) | 16GB~32GB8 | 10GB~24GB510 | Gemma 3 12B/27B, Phi-4 14B, Deepseek R1 32B67 |
| 초대형 (70B 이상) | 64GB 이상11 | 48GB 이상 (또는 GPU 2개)10 | Llama 3.3 70B, Qwen2.5 72B6 |

핵심 통찰:
• VRAM 한계: 모델이 GPU의 VRAM 용량을 초과하여 시스템 RAM을 사용하게 되면(오버플로우), 성능이 5~20배 이상 저하됩니다.2...
• 양자화(Quantization): 모델 크기를 줄여 성능 손실을 최소화하면서 VRAM 요구량을 낮추는 기술로, Q4_K_M 설정이 속도와 품질 사이의 표준적인 균형점으로 추천됩니다.14...
--------------------------------------------------------------------------------
2. 안드로이드 기기에서의 AI 구동 (Termux 활용)
모바일 기기에서도 Termux라는 리눅스 터미널 에뮬레이터를 통해 Ollama와 최신 모델(Llama 3.2 등)을 구동할 수 있습니다.1718
• 설치 방법: 구글 플레이 스토어 버전은 업데이트가 중단되었으므로 GitHub나 F-Droid에서 최신 APK(v0.119 이상)를 내려받아야 합니다.1920
• 설치 명령어:
    1. 저장소 권한 부여: termux-setup-storage2122
    2. 패키지 업데이트: pkg update && pkg upgrade22
    3. Ollama 설치: pkg install ollama (최신 Termux 저장소에 포함됨)2324
    4. 서버 실행: ollama serve & 후 ollama run llama3.2:1b2325
기기별 적합성 (갤럭시 노트 10 시리즈 중심):2627
• 갤럭시 노트 10 (RAM 8GB): 1B~3B 규모의 가벼운 모델(Llama 3.2 1B, Gemma 3 1B 등)이 시스템 안정성 면에서 가장 적합합니다.2829
• 갤럭시 노트 10+ (RAM 12GB): 추가 RAM 덕분에 3B~4B 모델(Phi-3 Mini 등)을 더 안정적으로 구동하며, 마이크로 SD 슬롯을 통해 대용량 모델 파일 저장이 용이합니다.27...
--------------------------------------------------------------------------------
3. 성능 최적화 및 주요 쟁점
삼성 RAM Plus 기능31
• 개념: 기기 저장 공간의 일부를 가상 메모리(zRAM/Swap)로 사용하여 더 많은 앱을 백그라운드에 유지하는 기능입니다.3132
• AI 구동 시 영향:
    ◦ 저사양 기기: RAM 부족으로 인한 앱 강제 종료(OOM)를 방지하는 버퍼 역할을 하여 안정성에 도움을 줄 수 있습니다.3133
    ◦ 고사양 기기(노트 10+ 등): 데이터를 압축/해제하는 과정에서 CPU 자원을 소모하므로, AI 연산 속도를 높이려면 기능을 끄거나 최소값(2GB)으로 설정하는 것이 권장됩니다.27...
NPU 및 GPU 가속 현황
• Snapdragon X Elite/8 Gen 3: 현재 Ollama는 모바일 NPU를 완전히 활용하지 못하고 주로 CPU 연산에 의존하고 있습니다.3637 하지만 OpenCL이나 Vulkan을 통한 GPU 가속 지원이 실험적으로 진행 중입니다.3839
• AMD Ryzen AI 300: 전용 가속기와 VGM(가변 그래픽 메모리) 기능을 통해 노트북 환경에서 매우 뛰어난 추론 속도와 낮은 지연 시간을 보여줍니다.40...
추천 경량 모델 (Sub-7B)4344
1. Phi-3 Mini (3.8B): 마이크로소프트의 합성 데이터로 훈련되어 논리력과 코딩 능력이 우수함.4546
2. Llama 3.2 (1B/3B): 모바일에 최적화되어 속도가 매우 빠르고 다국어 지원이 뛰어남.4748
3. Gemma 3 (1B/4B): 구글의 모델로 텍스트와 이미지를 동시에 처리하는 멀티모달 능력을 갖춤.749
비유적 이해: 로컬 LLM을 실행하는 것은 책(모델 데이터)을 읽고 이해하는 과정과 같습니다. VRAM은 책을 펼쳐두는 책상과 같아서, 책상이 좁으면(VRAM 부족) 책을 계속 넣고 빼느라(시스템 RAM 스왑) 속도가 매우 느려집니다. 따라서 기기의 책상 크기에 맞는 적절한 크기의 책(양자화된 경량 모델)을 선택하는 것이 핵심입니다.50

---

## 📚 인용 정보

> 이 문서에는 **62개**의 인용이 포함되어 있습니다.
> NotebookLM에서 각 번호를 클릭하면 상세 출처를 확인할 수 있습니다.
